我还没有那么多测试Starling-7b-beta（前几天我玩过一些，这绝对是现在最好的7B车型之一）。我已经测试了许多其他人，也测试了良好的7B模型，例如Mistral-7b-Instruct-V0.2和Mistral Base 0.1的微调。我目前的最爱之一是7b-kunoichi-dpo-v2，这也是一个非常好的模型，我认为甚至比Starling更好。

**然而**，7b模型通常没有与大型模型（例如34b和70b）相同的深度，连贯性和知识。最近，我负责各种尺寸（7b，10.7b，34b和70b）的LLM，以编写短篇小说和假设的事件，尽管所有这些事件或多或少都做得很好，但它仍然是34B和70B型号这写了最有趣，最迷人的故事 /文本，因为它们更连贯，逻辑和理解细节。

至于编程，对于简单的用例，例如在JavaScript中编写功能的帮助，我认为7B型号非常好。但是，更复杂的编码任务使7B模型挣扎。例如，我编写了一个带有故意内存泄漏的小型C ++程序，如果代码中有任何内存泄漏，则询问了各种7B模型和较大的混音模型，并且只有Mixtral指出了内存泄漏。我认为7B无法找到内存泄漏，因为它需要对代码进行整体掌握，而小型7B型号缺乏连贯性。
我尝试将其用作针对GPT-3.5的简单写作测试，并且令人信服地失去了它。我想说的是，它的一代人在7b中的平均水平确实高于平均水平，但它像7bs一样弄乱了角色的细节（我在哈利·波特和赫敏之间写了一个简单的卷积，它给了哈利·蓝眼睛和赫敏红头发）。请注意，这两种模型都以同样的“ Blabla”序列结束，这给了我癌症。

我认为它之所以如此之高的部分原因是因为周围的所有其他大型模型（克劳德2/2.1，混音，3.5-turbo，gemini-pro，mistral-next）都因被过度审查而臭名昭著。我认为他们提供的无数拒绝可能正在压低他们的分数……这是一个重要的事情，因为这不是真正的“哪个模型最聪明”，而是“哪种模型喜欢最好的人”  - 尽管您可以说一个拒绝回答的模型与任何可能的愚蠢。

鉴于Claude-3和GPT-4仍在审查中，这意味着它们比其他基线要好得多。
它适合7B，距离GPT-4很近。另外，您要查看的分数不是线性的，因此1128分数可能是1129的10倍
我要说的是，该排名对于完成人类语言任务的能力是正确的。但不是为了编码或科学问题解决，其性能更像ChatGpt-3.5。

这是我尝试过的最好的本地托管模型。甚至比一些70B型号更好。如果我的工作不支付Chatgpt Pro和Github Copilot，那么Starling-LM-7B将是我的日常驾驶员。
我得到了编码的答案，它们完全幻觉了。我正在使用竞技场。当我看到模型时，我就像一个7B，哦，好。
过去，我被这么多模型所燃烧，据称我比Chatgpt 3.5更好。

首先，让我们解决分数问题，您不划分分数，而是减去分数。1158-1127 = 31，使用它可以推断出GPT-4-0613将赢得54％的时间，而Starling 46％。目前还不清楚，因为有多个ELO系统，并且聊天机器人竞技场对细节的预期不是很好。

但是，该模型很好，但是我怀疑很多要点来自创意写作，而不是事实准确性。
Starling是我去当地LLM的去。太棒了。我不知道为什么更多的人不谈论它。我强烈建议您尝试一下。
我尝试了它，但没有比基于OpenChat-3.5-0106更好的发现。仍然令人惊叹的7b型号，如果您更喜欢开场白或Starling-LM-Alpha的详细性。
是否有一个指标有助于评估哪种模型最适合RAG使用特定工程学科的模型？我计划进行免费的应用程序，该应用程序可以访问学生或专业人士可以使用的数百本教科书，并独自一人找到信息而无需浏览书籍。我以前曾制造过商业产品，目前在LLM上进行了培训。如果我可以减少反复试验，这将节省时间。😁
这在图表上在哪里？
https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm
我尝试了它，但仍然喜欢α。它太字面了，在我的测试中没有进行任何头脑风暴。这可能是这种字面上的思想，而且没有受到审查，人们喜欢的。
我们介绍了Starling-7B，这是一种开放的大型语言模型（LLM），该模型通过AI反馈（RLAIF）训练。

https://starling.cs.berkeley.edu/
尝试了抹布。表现不佳。OpenChat 3.5好多了
嗯，我不知道聊天机器人竞技场如何工作。我的意思是我知道ELO等级并显示多个答案，然后让用户对它们进行评分，但是问题/答案数据集公开并由LMSYS决定，或者用户可以输入他们想要的任何内容，然后对答案进行评分？
如果这是一个（公开可用的）问题/答案数据集，那么我想模型创建者会在培训中过度拟合该数据集以排名更高吗？
这个排名网站在哪里？
它有用于编码llm的部分吗
令人失望的是，Starling没有GPTQ变体。我想念布洛克。尽管排行榜往往不是聊天/角色扮演之类的事情，但要尝试GGUF并进行测试并进行测试。

编辑：因此，我在下载时要介绍模板详细信息，忍不住注意到这一点：

> **重要：请使用下面为模型提供的确切聊天模板。否则，表演会降低。在极少数情况下，模型输出可以冗长。请考虑设置温度= 0以使其更少。**

我从未见过模型说考虑将温度设置为零，为什么？

编辑：好的，在测试后，我看到了它们对模型是冗长的意思，它确实喜欢说话并消耗大量的描述上下文。

它确实在描述事物方面确实做得很好，它通常会陷入重复的循环中，从而浪费了以前答复中的上下文重复行。

这也很遗憾，因为如果不是为了重复性，那就太好了，而且仅出于这个原因，我真的不能为聊天/角色扮演目的而建议。

希望有人对其进行修复或将其与有助于纠正此事的东西合并。如果他们这样做，我可能会使用更多，但是无论哪种方式，我都会继续尝试使用不同的用例和设置。
那是很好的。胜过Mistral的微调
您可以在本地运行Claude吗？离线？
我尝试了使用Ollama并提出了不同的问题，它只是继续了文字，从未回答过。
我不知道它的评分很高。很好，但是与GPT 3.5相比，没有什么。它有时会出现奇怪的语法错误，而且通常不会听你的。这些竞技场中的一些得分很擅长表面上欺骗人们。不过，仍然比其他LLM测试板更好。
我的测试不好。总是重复自己。
编码的唯一真正帮助是使用多种型号，我倾向于在qwen2 14b或Starling 7B上发出快速问题，那么如果看起来像是一个很好的答案测试。我正在使用一个3090。

然后，如果需要更多，请移至GPT4，Claude，Gemini，Copilot。

不再觉得本地模型越来越较低，因为它全都是通过正确的培训数据来击中该模型。
我使用了更好的7B型号。与其他7b型号的性能相比，它的表现并没有给我留下深刻的印象。具体而言，它避免回答其知识的历史事件的细节。
热门：LMSYS排名本质上是毫无价值的。尽管它确实支持更长的对话，但绝大多数选票来自一个输入>一个回应。从如此短暂的对话中判断LLM的质量根本是不可能的。
它比最近基于Mistral 7b的Nous Hermes Pro 2.5更好。我比较了几个提示，结果更准确，更好的简洁性和更少的幻觉。但仍然远离GPT4（我使用GPT4 Turbo作为比较）。
也许擅长对话，但不能用于基准（MMLU，GSM8K等）
我不相信拥有Gemini Pro如此高的基准
对于大多数具有当前技术的人来说，34B可能是可行性的边界，除了运行外，一切都很昂贵，而且价格昂贵。

浮动3.5-Turbo的20b图总是让我感到非常有趣，它足够大，如果饱和，但也足够小，可以保持实用。最初的美洲驼的跳过13B和33B之间的跳过可能是一个巨大的错误。
您将推荐哪种最小模型用于摘要任务？输入是〜400-500字的书面文本
我已经完成了以前使用的13B型号移动到7b-kunoichi-dpo-v2，但这很可能。仅仅是因为我目前不可能运行34/70。我想如果我有能力，我会以34/70的更改。
一个“愚蠢”的小型模型的完美示例，赢得了更大的Hyber censonss型号。因此，人们对较小的模型进行了投票，因为与审查模型的道德拒绝相比，它实际上也给出了一个答案。
>请注意，这两种模型都以同样的“ Blabla”序列结束，这给了我癌症。

增加温度。
商业模型使用许多独立模型的优化，例如预/后处理提示，抹布，短期内存和其他许多东西。这就像将带有完整研讨会的汽车机械师与内衣的车间进行比较。
TBF，拒绝回答的模型只有与之合作的挑战。您仍然可以越狱，以从他们的智能中获得所需的有用性。没有任何提示会使7b型号更聪明，但是
Elo的配制使得100分的差距是获胜的三分之一。1分差距意味着几乎有50/50的获胜机会 - 即，模型在感知的质量上几乎完全相同。

但是，ELO在答案质量和审查制度中有些误导。许多商业模型将拒绝回答一些问题，这些问题较少审查的开放模型会回答。（并且某些商业模型不如其他商业模型受到审查）。这导致了不拒绝回答问题的模型的“胜利”。

因此，这里的31点差距的解释是，对于许多问题，模型的表现类似，并且很可能有合理数量的问题封闭模型审查该模型没有。
您会说这比GPT-3.5 Turbo好吗？排行榜上方的许多地方是否只是因为该模型还没有在足够的时间进行测试 /投票？
[在EQ-Bench上]（https://i.imgur.com/uiikyee.png）

我同意对于7b *来说似乎很强大 *。但是，对复杂推理能力（例如MAGI）更具歧视性的基准将显示SOTA模型与前7B模型之间的分数分离。竞技场ELO不能很好地量化能力差异。
请注意，LMSYS使用基本的ELO算法，您可以在其COLAB [笔记本]（https://colab.research.google.com/drive/1kdwokpjirktmpo_p1wokp1wbyfnfnfnfnfnfiqxwqquwwqquwhqquwhwunpto = scrollto = o_cpbkgebkgebkrk）中查看源代码。
有用的帖子，但我认为他们对细节非常重要。不仅代码（https://colab.research.google.com/drive/1kdwokpjirktmpo_p1wbyfnfnfiqxwqquwh as u/nekoofneko提供）），而且他们还写了一个关于如何使用bradly-terly-terry of Elo的博客，他们的自举置信区间等等：https：//lmsys.org/blog/2023-12-07-leaderboard/
31如何产生54％？我只是好奇，因为我在这里不熟悉得分。
我几个月前尝试了Starling，并立即看到了潜力，但该模型不知道何时停止产生。它只会重复自己，直到完成整个上下文。
是的，这是我在演示文稿中使用的内容来显示本地LLM。

相比之下，Mistral是如此废话。
您会建议我盯着其他7B Mixtral型号吗？

我只是在建立一个有趣的周末项目，该项目获取段落，并从给定的标签列表中列出标签。像Mistral大型和Claude这样的模型做得很好。我正在考虑使用一些7B型号。
该排行榜是由人群来源的结果制成的，用户在其中提示了2个模型，然后选择哪种响应最好。在进行另一个用户测试之前，不会显示模型。DBRX（B或I）将不得不等到足够的人进行测试才能出现在此排行榜上。排行榜是：拥抱脸上


TLDR：排行榜来自实践测试，新的DBRX模型可能需要一段时间才能放置它。
您可以输入任何想要的问题并对模型进行评分

它不是由LMSS确定的
您可以在CPU上进行GOTQ大约需要30分钟。
不，排行榜使用其API。
（我从其他答复中粘贴了这一点）即使我曾经经历过一次。因此，由于不正确的提示模板设置，这可能是（可能是）。这是您在LM Studio中进行的操作：

￼

https://preview.itd.it/ztdfi46mo5rc1.jpeg?width=518＆format = pjpg＆format = pjpg＆auto = webp&s = ec22c94fe9b602b602bdf5f5f5f5f5f5f077848484848fdbeca39b699b69b602d8e

￼

用户消息前缀应为：

GPT4正确的用户：

后缀应该是：

<| end \ _of \ _ turn |> gpt4正确的助手：

停止令牌：<| end \ _of \ _turn |>
我什至没有经历过一次。因此，由于不正确的提示模板设置，这可能是（可能是）。这是您在LM Studio中进行的操作：

https://preview.redd.it/pppbmazvn5rc1.jpeg?width=518＆format = pjpg＆auto = webppumpppumpppumpppp&s=f3ec222a951758971555555555313CD16FC5876C876C8122B

用户消息前缀应为：

GPT4正确的用户：

后缀应该是：

<| end \ _of \ _ turn |> gpt4正确的助手：

停止令牌：<| end \ _of \ _turn |>
是的，完全同意。诸如Mistral 20b之类的好20b型号将是理想的景点，尤其是对于具有中档PC的用户而言。很可能比Mistral 7b更好**，而且跑步仍然不那么沉重。

不过，一些好消息，新的Llama.cpp量化 + imatrix Tech使得可以在质量良好的中端PC上运行70B型号。我的设置是一台非常糟糕的中端PC，现在我可以运行[Midnight-Rose-70b-v2.0.3]（https://huggingface.co/artefact2/midnight night-rose-70b-v2.0.3-gguf）使用Imatrix IQ3 \ _xxs Quant。在我的用例中，它创建了非常高质量的输出。只有缺点，它仍然很慢，但是我可以要求它写一个很酷的故事，同时去喝咖啡，当我回来时，我在我的前面有一段令人兴奋的高质量阅读。
> 34b可能是大多数具有当前技术的人的可行性边界，除了运行的一切都很昂贵

我在CPU上运行了70B型号（量化），对结果感到非常满意。它比GPT4慢或在GPU上运行，但它足够快满足我的需求。
除非像多路星这样的东西是标准的本地型号，否则不会赶上庞大的专有。
在24GB卡上的Q4/Q5上有34B非MOE非常出色。

但是我们可以使用Moe做得更好 - 例如总共有96B的MOE，仅在12B时仅激活2-3个，每个MOE都量化为Q4或Q5，该量子应在24GB卡或带有32+ GB RAM的Mac上运行得很好。
我现在只是质疑一切。DBRX是一个很好的例子。甚至都不与chatgpt3.5竞争。落后于Chatgpt4。

评估也是对其大哥的量化版本的怀疑。我怀疑，每个量化模型也需要重新运行评估。
我在Lymsys竞技场上进行了比较。因此，只需库存设置即可。
您是说商业聊天机器人？API呼叫中没有这些事情发生。
我应该补充说，此比较是仅使用Lymsys Arena聊天机器人进行的，您可以直接比较任何两个型号。因此，我不知道他们的设置是什么，但我会认为他们应该尽可能接近人们在竞技场排行榜中获得的实际结果。
他们有一种单独的方式来表明审查答案，这很酷。因此，您可以选择一个答案（或两者兼而有之），而不是选择答案更好，而不是ELO等级，而是影响单独的“审查级”。
他们应该真正将审查和未经审查模型的评分分开。您可以在聊天界面中为未经审查的审查与未经审查的切换，以确定哪些模型正在盲目测试。
赢得事物的差距和机会是否取决于K因素？
可能是因为GPT3.5被审查到没有用的地步。因此，这取决于您使用的方法。
我很难找到它，肯定的是，现在您指出，我绝对可以理解他们在做什么。但是，您需要找到笔记本，阅读笔记本并了解编程。或阅读博客。它应该在网站上更暴露。
“玩家A”的预期分数为s \ _a = 1/（1 + 10 \^（（（R \ _b-r \ _a）/400），其中r \ _b和r \ _a是他们的ELO排名。现在将这些分数替换为我们的S \ _a为0.5444946。ERGO上面提到的54％。（和1-54％的人获得46％）。Elo等级Wikipedia文章的[数学详细信息]（https://en.wikipedia.org/wiki/elo_rating_system_system #mmathematical_details）部分具有更多详细信息。
您可以轻松地在Wikipedia上查找ELO算法，因为它简单明了。
型号旨在通过生成唯一的终端tokenhowever来发出信号，如果聊天界面未正确配置，因为您没有设置正确的停止令牌，它可能会继续无限期地生成文本。
其中一些可能是由于提示。我不记得Starling的及时格式，但稍后会尝试一下。
我绝对建议尝试。Starling和OpenChat都非常擅长遵循说明和推理，而且他们进行了非常多功能的培训，这使他们易于推荐。但是，当然，他们并不是在克劳德（Claude）或米斯特（Mistral）。
我完全被混音所困扰。也许角色玩家正在玩乐，但是如果我可以使用Starling，它基本上是毫无用处的。
然后我想伯克利在这款7B模型上做得很好:)
谢谢你的信息。我有时间时会尝试
如何 ？如果您不介意分享您的规格是什么？
同样，但是我将几层卸载到我的8gig vram上，以加快一代的加快。我主要使用70B型号来生成故事，我不需要为此目的进行实时聊天，很乐意等待几分钟才能完成。
你能给我一个关于如何做的事情吗？这一切的新手
34B在4位的32GB系统RAM中拟合，例如，如果您考虑其他运行的东西。这可能是如今的普通玩家钻机或繁重的办公室工作站所拥有的。除此之外，任何人都必须构建一个在该潜艇之外的LLM特定系统。
好奇您对70B的T/S是什么，CPU的用例（RP除外）是什么？我可以分配28GB VRAM无头运行，但是我被其余的芯片瓶装，当我尝试使用miqu Quants时，我的芯片瓶装为2t/s。
> API呼叫中没有任何事情进行。

大胆的声明，您将如何证明这一点？这是我从客户那里得到的一个大问题，也是一个常见的投诉，您无法确定使用API时在与之交谈的确切问题。GPT4可能是一个全新的型号，或者只是3.5的调整版本，我们永远不会知道，那么您怎么知道它不会使用抹布或经过思考的API调用？

另外，什么将通过API实现使客户访问Bare LLMS？最糟糕的总体表现和所有的负面效果，就像更多的幻觉一样。从业务角度来看，这只是一个不好的决定。
审查不是二进制的。模型具有不同程度的审查制度和不同的主题，并以不同的方式。有些人会拒绝回答，有些会缩短响应，有些人会暗示其他建议。

它并不明确。
我的理解（这可能是完全错误的:)）是K因子确定评级更新的波动性，不会影响胜利概率，也不会影响差距。

https://stats.stackexchange.com/questions/448621/does-the--elo-k-factor-aftect-winning-probibilities

他们使用4的K因子进行LLM模型比较，

https://colab.research.google.com/drive/1kdwokpjirktmpo_p1wbyfnfnfiqxwqquwh

FIDE K因子（国际象棋评级），

> k = 40，对于刚获得评分列表的球员，直到他完成至少30场比赛的活动
> k = 20，只要玩家的评级保持在2400以下。
> k = 10，即使评级下降到2400以下，一旦玩家发布的评级已达到2400，随后仍处于该级别。
> k = 40个球员直到18岁生日，只要他们的评级保持在2300以下。

https://ratings.fide.com/calculator_rtd.phtml
太好了哈哈，他们在做什么真的很可悲。我希望较小的开源模型实际上会击败GPT-4 ...
非常感激！谢谢！
啊，我明白了。我认为这是排行榜的一些自定义评分方法哈哈:)
我想这就是为什么GPT4强迫写“结论”的原因
这可能是一个较旧的版本。我将不得不窥视lmstudio，看看它是哪个并报告或编辑评论
谢谢你！
模型有很多用途，而没有说明您使用的是噪音。
i5 CPU，32GIG RAM，RTX 3060 GPU，带8 GIG VRAM。我主要在CPU上运行Midnight-Rose-70B，并在GPU上卸下几层，以加快生成速度。
抱歉，我是新手。我可以问一下您如何将一些图层卸载到VRAM上吗？感谢您是否可以分享与之相关的论文。提前致谢。
只需转到Llama.cpp或Oolama Git存储库，然后跟随Readme，这很简单。
API客户重视可重复性，这就是为什么他们固定型号版本的原因。此外，您描述的所有降级模型提供商能够可靠定价产品的能力，因为它们是可变的预处理和后处理步骤，可能会很重要，并且随着时间的推移，支撑抹布/ETC框架的任何变化都会破坏可重复性，这将是非常非常重要的对客户很明显。


可能会有较小的服务在API调用中可以做到这样的奇怪的事情，但是如果OpenAI/Anthropic/Mistral正在这样做，那么现在就很清楚了。
尝试不越狱，ya骗子。

https://i.imgur.com/gujuzgj.png
诸如chatgpt之类的通用目的

我可以让Chatgpt开箱即用。
>到目前为止，很清楚。

再次您会注意到？对API和后端的未经通知和无证更改是OpenAI论坛上非常普遍的投诉，因此可靠性显然不是他们的主要优先事项。对于那些与API进行广泛合作的人，这应该是显而易见的。

>退化模型提供商可靠定价其产品的能力

哦，您从事营销工作，这解释了很多。除了开玩笑，他们将尽可能高地为产品定价。简单地为运营成本增加健康的总和要容易得多，而且有利可图。
使用某些语言被认为是越狱的自定义说明吗？
在这种情况下，越狱还会是什么？
